{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113f8932",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "## Background reading:\n",
    "*Intro to Statistical Learning*: Chapter 9， Section 9.1, 9.2, 9.3, Support Vector Machine(Page 368-385)\n",
    "\n",
    "*Ethical Algorithm*: Chapter 2, Algorithmic Fairness, Bias by Analogy, (Page 57-63)\n",
    "\n",
    "## Dataset:\n",
    "**2 Attributes:**\n",
    "\n",
    "1. Words (Specific vocabularies that are feminine, masculine or neutral)\n",
    "2. Category\n",
    "\n",
    "Datasets comes from: https://link.springer.com/article/10.3758/BF03195592\n",
    "\n",
    "This tutorial aims at using SVM to classify whether a word is gender-biased. To deal with vocabulary in computer, background of Natural Language Processing is discussed here for you to read if you are interested in.\n",
    "\n",
    "## Essence of Data\n",
    "\n",
    "### Before Algorithms:Basic Concepts\n",
    "\n",
    "In this section, we will first introduce some basic concepts to help you understand what is support vector machine.\n",
    "\n",
    "**Hyperplane**\n",
    "\n",
    "Def: A subspace which has one less dimension than its space.\n",
    "E.g.,  In a 2-dimensional space, a hyperplane is in one dimension (a line); In a 3-dimensional space, a hyperplane is in two dimensions (a plane)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6eedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\75994\\anaconda3\\lib\\site-packages\\rpy2\\robjects\\packages.py:367: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "#This is used here to allow us to use both Python and R in Jupyter Notebook\n",
    "# Before Running it, make sure that the IR kernel is installed to jupyter. You can also run these codes on r-studio for r languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e01f45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: ggplot2\n",
      "\n",
      "R[write to console]: Loading required package: plotly\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: 'plotly'\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from 'package:ggplot2':\n",
      "\n",
      "    last_plot\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from 'package:stats':\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from 'package:graphics':\n",
      "\n",
      "    layout\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <span>BoolVector with 1 elements.</span>\n",
       "        <table>\n",
       "        <tbody>\n",
       "          <tr>\n",
       "          \n",
       "            <td>\n",
       "                   1\n",
       "            </td>\n",
       "          \n",
       "          </tr>\n",
       "        </tbody>\n",
       "        </table>\n",
       "        "
      ],
      "text/plain": [
       "<rpy2.robjects.vectors.BoolVector object at 0x000002C46E38DD00> [RTYPES.LGLSXP]\n",
       "R classes: ('logical',)\n",
       "[       1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before run the code, make sure that you have already installed ggplot2 and plotly packages. \n",
    "# Load the package for ggplot2, which is an excellent package to draw statistical graphs\n",
    "%R require('ggplot2')\n",
    "# Package plotly is used for creating 3-d plots\n",
    "%R require('plotly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a020f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a data frame to help understand the hyperplane in 2-d/3-d\n",
    "import pandas as pd\n",
    "df2 = pd.DataFrame({\n",
    "    'x':[0,1,1,2,2,3,3,4,4,4,5,5,6],\n",
    "    'y':[5.2,3.8,4.9,2.5,5.5,3,4,2.2,3.4,4.6,3,2.1,1.5]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5549d909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i df2\n",
    "df2$above_below=ifelse(df2$y+0.5*df2$x-5>0,1,0)\n",
    "df2\n",
    "ggplot(df2, aes(x=x,y=y,col=above_below))+\n",
    "       geom_point() + geom_abline(slope=-0.5,intercept=5,col='red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02074cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\n",
    "    'x':[1,2,2,3,4,6,5,3,3],\n",
    "    'y':[1,2,3,3,4,4,5,2,1],\n",
    "    'z':[3,4,-7,-2,-4,-3,-12,-7,-9]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d33c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df3\n",
    "df3$above_below=ifelse(df3$x+df3$y+df3$z>0,1,0)\n",
    "df3\n",
    "input1=as.matrix(seq(1,6,0.1))\n",
    "input2=as.matrix(seq(1,6,0.1))\n",
    "input3=as.matrix(-input1-input2)\n",
    "\n",
    "plot_ly(x=df3$x,y=df3$y,z=df3$z,type=\"scatter3d\", mode=\"markers\", color=df3$above_below)%>%\n",
    "add_surface(x=input1,y=input2,z=input3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760c094",
   "metadata": {},
   "source": [
    "The mathematical expression of the hyperplane in p-dimensions is:\n",
    "\n",
    "<div align = 'center'><font size = '6'>$\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_px_p=0$</font></div>\n",
    "\n",
    "From the graph above, the data points can be separated by a hyperplane in the space. When $\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_px_p>0$, the data points lie on one side of the hyperplane, while they lie on the other side when $\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_px_p<0$. It is reasonable to find out this natural classifier to do classification.\n",
    "\n",
    "**Maximal Margin Classifier and Support Vector Classifier**\n",
    "\n",
    "Margin: the perpendicular distance from the observation to the hyperplane\n",
    "\n",
    "Maximal Margin Classifier: Among all the possible hyperplanes that can separate the data into categorizes, select the one that come up with the maximal margin.\n",
    "\n",
    "**Support Vectors:**\n",
    "\n",
    "In the space, there are some observations points that have equal distance from the maximal margin hyperplane. For example, in this graph there are some support vectors lie on the dashed line, with their distances shown as arrows. These observations are called support vectors since they “support” the hyperplane (when the support vectors move a little bit, the hyperplane will move as well, while the movement of other observations does not affect the hyperplane)\n",
    "\n",
    "![Figure 9.3 From Textbook Page 372](./fig/SVM&WE/fig1.png)\n",
    "\n",
    "**Support Vector Classifiers**\n",
    "\n",
    "Sometimes the datasets cannot be separated perfectly by the hyperplane. In such case, support vector classifier is considered, which is a more generalized case of maximal margin classifier. We allow some of the observations to be on the wrong side of the margin/hyperplane. Details of the mathematical expression: page 375.\n",
    "\n",
    "![Figure 9.6 From Textbook Page 376](./fig/SVM&WE/fig2.png)\n",
    "\n",
    "Left: observation 1 and 8 are on the wrong side of the margin\n",
    "Right: observation 11 and 12 are on the wrong side of the hyperplane\n",
    "\n",
    "**Support Vector Machine:**\n",
    "\n",
    "Sometimes the datasets cannot be separated by using a linear hyperplane, even we allow some of the observations to be on the wrong side. For example, it is impossible to build up a classifier by using a linear boundary for the data below. Thus, we need some techniques to increase the complexity and the non-linearity of the model to make good predictions. The technique is called kernel.\n",
    "\n",
    "![Figure 9.8 From Textbook Page 379](./fig/SVM&WE/fig3.png)\n",
    "\n",
    "What is **kernel**?\n",
    "\n",
    "By using some kernel function to transform the data into, for example, higher dimensions, to ensure that the inner product is the same as the previous one, then to make it separable in the new case.\n",
    "\n",
    "Examples of kernel functions:\n",
    "Polynomial, Gaussian, RBF, Sigmoid, Radial, etc.\n",
    "\n",
    "**Which kernel to choose?**\n",
    "\n",
    "This is a very tricky problem and could be one of the drawbacks of SVM, since there are no good rules to follow. Sometimes you should try a lot of different kernel functions to reach out one best model. For example, for the data graph above, both kernel function of polynomial of degree 3 (left) and radial (right) works well.\n",
    "\n",
    "![Figure 9.9 From Textbook Page 383](./fig/SVM&WE/fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b87acd",
   "metadata": {},
   "source": [
    "### Before Application: Background of Natural Language Processing\n",
    "\n",
    "In this section, we will cover topics related to natural language processing(NLP)\n",
    "\n",
    "**Word Processing**\n",
    "\n",
    "\tWhen dealing with the language (known as Natural Language Processing), unfortunately the computer cannot handle the characters/vocabularies and does not understand the meaning of them (Remember when the computer processes the characters, it will transform them into digital data by some encoding form such as Unicode or UTF-8). Consequently, it is necessary to translate the language into digital data before the computer can analyze. The most common techniques include one-hot encoding and word embedding.\n",
    "\n",
    "Here are two useful articles related to the background:\n",
    "\n",
    "**Power of NLP**\n",
    "\n",
    "https://hbr.org/2022/04/the-power-of-natural-language-processing\n",
    "\n",
    "**NLP and Google Translate**\n",
    "\n",
    "https://www.kdnuggets.com/2017/09/machine-learning-translation-google-translate-algorithm.html\n",
    "\n",
    "**What is word embedding?**\n",
    "\n",
    "\tWhen doing the natural language processing, associate a vector with a word where the vectors are learned from data. i.e., generate vectors that encodes the meaning of the words so that the words that are closed to each other in the space that we are interested are similar in their meanings. For example, in a space, we have a “gender” vectors and “plural” vectors. By adding a “female” vector to the vector “king/actor”, we obtain the vector “queen/actress”; by adding the “plural” vector we will obtain “kings/actors”\n",
    "\n",
    "![Words on different axises](./fig/SVM&WE/fig5.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "40f1c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_we = pd.DataFrame({\n",
    "    'gender_axis':[1,1,3,3],\n",
    "    'plural_axis':[1,3,1,3]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fb31f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%R -i df_we\n",
    "\n",
    "ggplot(data=df_we) +\n",
    "    geom_point(mapping = aes(x = gender_axis, y = plural_axis), shape = 4, size = 6, col = \"red\") +\n",
    "    annotate(\"text\", x = 1.2, y = 1.2, label = \"king\") +\n",
    "    annotate(\"text\", x = 3.2, y = 1.2, label = \"queen\") +\n",
    "    annotate(\"text\", x = 1.2, y = 3.2, label = \"kings\") +\n",
    "    annotate(\"text\", x = 3.2, y = 3.2, label = \"queens\") +\n",
    "    scale_x_continuous(limits=c(0,4)) +\n",
    "    scale_y_continuous(limits=c(0,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01148885",
   "metadata": {},
   "source": [
    "https://www.ibm.com/blogs/research/2018/11/word-movers-embedding/\n",
    "\n",
    "![Words vectors in the space](./fig/SVM&WE/fig6.png)\n",
    "\n",
    "**Why word embeddings?**\n",
    "\n",
    "\tWhen doing natural language processing, the computer itself does not understand the real meaning behind the vocabularies. It only processing the digital data. Thus, when dealing with languages (or any other non-digital data), we should figure out a way to transform the data into digital data (encoding) so that the computer can understand and process with it, then we get the result by decoding it.\n",
    "\tThere are several different methods to transform the language. Another popular one is the one-hot encoding, which is also a common way. It consists of associating a unique integer index with every word and then turning the index into a binary vector of size of the vocabulary, while the other to be zero. i.e., the vector will be zeros except for that index to be 1. \n",
    "    \n",
    "![Examples of one-hot encoding and Word Embeddings](./fig/SVM&WE/fig7.png)\n",
    "\n",
    "\tHowever, you may think about the downsides of word embeddings. Imagine if there are some neutral vocabularies that do not imply any gender, but it is predicted to be a gender-specific word (which is a bias! Because the word itself does not have any gender-bias) For example, one model may encodes “muscles”, “Programmer” and “game” with vectors that have more “male” values of projection on the gender axis. Sometimes the insufficient of the data will aggravate the bias further.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724c39a",
   "metadata": {},
   "source": [
    "### Example of Codes\n",
    "\n",
    "In this section we will try to use word embeddings and SVM as tools to process with words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82aec7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, Import all packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2582df5",
   "metadata": {},
   "source": [
    "*To make a word-embedding model, we will use gensim package which is sophisticated. To simplify understanding, we will use a pre-trained model (Word2Vec) by Google that contains millions of words that are trained based on its corpus. \n",
    "\n",
    "*The pre-trained model is described here: https://code.google.com/archive/p/word2vec/. The model contains millions of words with their vectors (thousands dimensions) with enormous size. \n",
    "\n",
    "**The pre-trained model can be downloaded from here:https://code.google.com/archive/p/word2vec/\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "383af829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x17ef62910c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained model (It takes some time to run it cause the size is millions)\n",
    "# ! Download from website and store it\n",
    "word2vec_path = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "model = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
    "# Notice that model is a \"Word2Vec\" pre-trained model object here\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7ee88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wife</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Princess</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bridesmaid</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pregnant</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bride</td>\n",
       "      <td>Feminine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>Testosterone</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>Husband</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>Father</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>Brother</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Uncle</td>\n",
       "      <td>Masculine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Vocabulary   Category\n",
       "0            Wife   Feminine\n",
       "1        Princess   Feminine\n",
       "2      Bridesmaid   Feminine\n",
       "3        Pregnant   Feminine\n",
       "4           Bride   Feminine\n",
       "..            ...        ...\n",
       "595  Testosterone  Masculine\n",
       "596       Husband  Masculine\n",
       "597        Father  Masculine\n",
       "598       Brother  Masculine\n",
       "599         Uncle  Masculine\n",
       "\n",
       "[600 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('WordEmbeddings.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9f0a7f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'head' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     word_dict[key] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_vector(key)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mhead\u001b[49m(word_dict)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mlen\u001b[39m(word_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'head' is not defined"
     ]
    }
   ],
   "source": [
    "# Access the vectors of the model\n",
    "word_dict = dict({})\n",
    "for index, key in enumerate(model.key_to_index):\n",
    "    word_dict[key] = model.get_vector(key)\n",
    "\n",
    "# Print\n",
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80db635a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The size is too large. Filter those that are useful\n",
    "new_dict = dict((k, word_dict[k]) for k in dataset['Vocabulary'] if k in word_dict)\n",
    "len(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d4119ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595\n"
     ]
    }
   ],
   "source": [
    "# Settle Database\n",
    "\n",
    "x_all = list()\n",
    "for x in new_dict.values():\n",
    "    x_all.append(x)\n",
    "\n",
    "print(len(x_all))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad14e255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all = list()\n",
    "for y in dataset['Vocabulary']:\n",
    "    if y in new_dict.keys():\n",
    "        y_all.append(y)\n",
    "\n",
    "len(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "56880c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate into training and testing\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size = 0.2, random_state = 43960)\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b2d44dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start SVM classifier\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "030bd77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Accuracy\n",
    "svm_classifier.score(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "28ddebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Accuracy\n",
    "svm_classifier.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f1c15",
   "metadata": {},
   "source": [
    "## AI in the fabrics of society\n",
    "\n",
    "Notice that the accuracy of the training data is perfect, but it does not work on testing data. That is to say, when we use machine to classify whether a word is gender-based or not, it will make its own decision that is **sometimes not accurate** for future predictions (since **machine itself does not understand the meaning of vocabulary**)\n",
    "\n",
    "Although the accuracy sometimes is excellent for the machine learning algorithm, the problem is that sometimes it will cause some bias. For some vacabularies that is not a gender-based meaning, the algorithm will consider it to be a gender-related because of, for example, the occurance times of that words together with some other words(such as smoking may be considered to be a masculine word because it appears together with male more often than female)\n",
    "\n",
    "Here is an article regarding this gender-biased problem: \n",
    "\n",
    "Man is to Computer Programmer as Woman is to Homemaker? | by Sheldon Sebastian | Towards Data Science\n",
    "https://towardsdatascience.com/man-is-to-computer-programmer-as-woman-is-to-homemaker-e57b07cbde96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9cdb13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
